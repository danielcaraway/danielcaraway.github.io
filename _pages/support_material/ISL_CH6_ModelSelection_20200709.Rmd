---
title: "ISL_CH6"
output: html_document
---

# MODEL SELECTION
======================

```{r}
library(ISLR)
summary(Hitters)
```
### Oh no! Missing values!!

We'll quickly deal with this by 
1. Removing everything with "na" `na.omit`
2. Checking our work `sum(is,na(Salary))`

```{r}
Hitters = na.omit(Hitters)
with(Hitters,sum(is.na(Salary)))
```

## Best Subset Regression

DEF: Looks through all possible regression models of all different subset sizes and looks for the best of each size. 
PRODUCES: A sequence of models which is the best subset for each particular size

```{r}
library(leaps)
regfit.full=regsubsets(Salary~.,data=Hitters)
summary(regfit.full)
```

It gives best-subsets up to size 8 -- what if we want all variables? 

```{r}
regfit.full=regsubsets(Salary~.,data=Hitters, nvmax=19)
reg.summary=summary(regfit.full)
names(reg.summary)
plot(reg.summary$cp, xlab="Number of Variables", ylab="Cp")
which.min(reg.summary$cp)
points(10,reg.summary$cp[10], pch=20, col="red")
```


```{r}
plot(regfit.full, scale="Cp")
coef(regfit.full,10)
```

## Forward Stepwise Selection

NOTES:
* Base subset is agressive, looking at all possible subsets
* Forward stepwise is a greedy algorithm. Each time it includes the next best variable
But it produces a nested sequence. 
* Each new model includes all the variables that were before, plus a new one

```{r}
regfit.fwd=regsubsets(Salary~.,data=Hitters,nvmax=19, method="forward")
summary(regfit.fwd)
plot(regfit.fwd,scale="Cp")
```

## Model selection using a Validation Set

* Make  training and validation set so we can choose a good subset model
* NOTE: Slightly different from the book
* `train=sample(seq(263),180,replace=FALSE)` In English: We're going to create a sequence of numbers from 1-263. We are going to take a sample of 180 from that sequence


```{r}
dim(Hitters)
set.seed(1)
train=sample(seq(263),180,replace=FALSE)
train
regfit.fwd=regsubsets(Salary~.,data=Hitters[train,],nvmax=19, method="forward")
```

Now we will work out the validation errors on the remainder of the data!

* We will make predictions on the observations not used for training. (We know there are 19 models so we set up some vectors to record the errors)

* To get the right elements of x test, we have to index the columns by the names that are on the coefficient vector. 

```{r}
val.errors=rep(NA,19)
x.test = model.matrix(Salary~.,data=Hitters[-train,])
for(i in 1:19){
  # use coef function to extract the coefficients
  coefi=coef(regfit.fwd,id=i)
  
  # making our own prediction method
  # subset of columns of x test that correspond to the variables that are in this current coefficient vector
  # Then matrix mulitplied by the coefficient vector
  pred=x.test[,names(coefi)]%*%coefi
  
  # And now that we have our predictions, we can calculate the mean squared error
  val.errors[i]=mean((Hitters$Salary[-train]-pred)^2)
}

plot(sqrt(val.errors),ylab="Root MSE",ylim=c(300,400), pch=19,type="b")
points(sqrt(regfit.fwd$rss[-1]/180),col="blue", pch=19, type="b")

# pch=19 simply means round, full dot (on the plot)
legend("topright", legend=c("Training", "Validation"), col=c("blue","black"), pch=19)
```

### Because it's tedious, not having a `predict` method for `regsubsets` we'll write one!!

* `regsubset` has a component called `call` and part of `call` is a formula and we can extract the forumla through this little "incantation" here. 
Using the formula, we make a model matrix from it. Much like we did before. Then we extract the coefficient and do the matrix multiplication. 

* Now we have a function for reuse in our next section!

```{r}
predict.regsubsets=function(object, newdata,id,...){
  form=as.formula(object$call[[2]])
  mat=model.matrix(form,newdata)
  coefi=coef(object,id=id)
  mat[,names(coefi)]%*%coefi
}
```

# Model Selection by Cross-Validation

We will do 10-fold cross-validation. 

```{r}
set.seed(11)
# Sample from the numbers 1-10, each is going to be assigned a fold number
# We create a vector, 1 to 10, of length the number of rows of hitters 
folds=sample(rep(1:10,length=nrow(Hitters)))
folds
table(folds)
cv.errors=matrix(NA,10,19)
for(k in 1:10){
  best.fit=regsubsets(Salary~.,data=Hitters[folds!=k,],nvmax=19,method="forward")
  for(i in 1:19){
    pred=predict(best.fit,Hitters[folds==k,],id=i)
    cv.errors[k,i]=mean( (Hitters$Salary[folds==k]-pred)^2)
    
  }
}
# Now we process our output matrix
rmse.cv=sqrt(apply(cv.errors,2,mean))
plot(rmse.cv,pch=19,type="b")

```

In the video, it seemed to prefer models of 11 or 12 -- per our current graph, it seems to prefer models of 9. 


# Ridge Regression and the Lasso

* Alpha = 0 in Ridge
* Alpha = 1 in Lasso
* For alphas between 0 and 1 we get elastic net models (which are in between ridge and lasso)

Using a package `glimnet` which does not use the model formula language, so we have to set up an `x` and `y` (which is the predictor matrix and the response)

```{r}
library(glmnet)
# Predictor matrix
x=model.matrix(Salary~.-1,data=Hitters)
# Response
y=Hitters$Salary
```

First, we will fit a ridge-regression model. We do this by calling `glmnet` with `alpha=0` (see helpfile?). there is also a `cv.glmnet` function which will do the cross-validation for us

*RIDGE REGRESSION* is penalized by the sum of squares of the coefficients
Penalties put on the sum of squares of the coefficients (controlled by parameter lambda) Criterion for ridge regresion is residual sum of squares which is the usual for linear regression plus lambda times summation

*RIDGE REGRESSION ELI5*

From [here](https://www.reddit.com/r/statistics/comments/2i7h6v/eli5_ridge_regression/)

Let's say you're trying to predict future satisfaction using a number of inputs (age, income, gender etc.). Ridge regression penalizes large weights assigned to each input so that one input does not dominate all others in the prediction.

For example, a linear regression may find that income is the most important variable by far in determining future satisfaction (i.e. is assigned the largest weight in the linear regression). But maybe you have a small sample, or you think your sample is not representative, so you don't want income to be essentially the only input used to predict future satisfaction. The other inputs may have an effect, but their weights in a linear regression are so small relative to that of income that income is essentially the only input that matters in your prediction. You want to take into account all your variables in a more "even" way. Ridge regression will force all coefficients to be in a similar range so that you get a wider range of predictions that are not dominated by income only.

```{r}
fit.ridge=glmnet(x,y,alpha=0)
plot(fit.ridge, xvar="lambda", label=TRUE)
```

*NOTE* Unlike subset and forward stepwise regression, which controls the complexity of a model by restricting the number of variables, ridge regression keeps all the variables in and shrinks the coefficients towards zero. 


```{r}
cv.ridge=cv.glmnet(x,y,alpha=0)
plot(cv.ridge)
```


## Lasso 
Lasso similar to ridge, the only difference is the penalty.
FOr lasso we minimize the RSS plus lambda and there's a very subtle change. Insated of the sum of squares of the coefficients we penalize the absolute values of the coefficients. 

*NOTE* Lasso is doing both shrinkage and variable selection

```{r}
# Don't need alpha here because alpha defaults to alpha=1
fit.lasso=glmnet(x,y)
plot(fit.lasso,xvar="lambda", label=TRUE)
```

We can also plot "dev" instead of "lambda" -- This is the percentage of deviance explained or percentage of the sum of squares explained (which is the rsquared)

This plot changes the orientaton

```{r}
plot(fit.lasso,xvar="dev", label=TRUE)
```
An indication that maybe at the end of the path there is overfitting

TL;DR: There are different ways of plotting the coefficients and they give you different information about the coefficients and about the nature of the path. 

```{r}
cv.lasso=cv.glmnet(x,y)
plot(cv.lasso)
```


### THIS PICKS THE COEFFICIENTS FOR US

```{r}
coef(cv.lasso)
```

Suppose we want to use our earlier train/validation set to select the `lambda` for the lasso?

```{r}
lasso.tr=glmnet(x[train,], y[train])
lasso.tr
pred=predict(lasso.tr,x[-train,])
dim(pred)
rmse=sqrt(apply((y[-train]-pred)^2,2,mean))
plot(log(lasso.tr$lambda),rmse,type="b",xlab="Log(lambda)")
lam.best=lasso.tr$lambda[order(rmse)[1]]
lam.best
coef(lasso.tr,s=lam.best)
```







# GLOSSARY:

*RSS (Residual Sum of Squares)* A residual sum of squares is a statistical technique used to measure the variance in a data set that is not explained by the regression model. PG. 62 in ISL

*Ridge Regression* penalizes potential over-representation -- for example, if the sample size is small or maybe not representative of the population as a whole, Ridge Regression will penalize the large weights associated with that potential over-representation 

