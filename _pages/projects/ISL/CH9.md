---
title: "ISL"
permalink: /projects/ISL/CH9
---

[CODE HERE](https://courses.edx.org/asset-v1:StanfordOnline+STATSX0001+1T2020+type@asset+block/ch9.html)

[The Math Behind SVMs](https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/)

## OUTLINE FROM LECTURES

### 9.1 Optimal Separating Hyperplanes

There is no probability model -- it just looks for a hyperplane that separates the classes in a direct way

QUESTION
If ğ›½ is not a unit vector but instead has length 2, then âˆ‘ğ‘ğ‘—=1ğ›½ğ‘—ğ‘‹ğ‘— is

ANSWER
twice the signed Euclidean distance from the separating hyperplane âˆ‘ğ‘ğ‘—=1ğ›½ğ‘—ğ‘‹ğ‘—=0

EXPLANATION
We know ğ›½â€²=12ğ›½ has length 1, so it is a unit vector in the same direction as ğ›½ . Therefore, âˆ‘ğ‘ğ‘—=1ğ›½ğ‘—ğ‘‹ğ‘—=2âˆ‘ğ‘ğ‘—=1ğ›½â€²ğ‘—ğ‘‹ğ‘— , where âˆ‘ğ‘ğ‘—=1ğ›½â€²ğ‘—ğ‘‹ğ‘— is the Euclidean distance.

### 9.2 Support Vector Classifier

### 9.3 Feature Expansion and the SVM

### 9.4 Example and Comparison with Logistic Regression

### 9.5. SVMs in R
