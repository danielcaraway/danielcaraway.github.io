---
title: "ISL"
permalink: /projects/ISL/CH9
---

[CODE HERE](https://courses.edx.org/asset-v1:StanfordOnline+STATSX0001+1T2020+type@asset+block/ch9.html)

[The Math Behind SVMs](https://shuzhanfan.github.io/2018/05/understanding-mathematics-behind-support-vector-machines/)

## OUTLINE FROM LECTURES

### 9.1 Optimal Separating Hyperplanes

There is no probability model -- it just looks for a hyperplane that separates the classes in a direct way

QUESTION
If 𝛽 is not a unit vector but instead has length 2, then ∑𝑝𝑗=1𝛽𝑗𝑋𝑗 is

ANSWER
twice the signed Euclidean distance from the separating hyperplane ∑𝑝𝑗=1𝛽𝑗𝑋𝑗=0

EXPLANATION
We know 𝛽′=12𝛽 has length 1, so it is a unit vector in the same direction as 𝛽 . Therefore, ∑𝑝𝑗=1𝛽𝑗𝑋𝑗=2∑𝑝𝑗=1𝛽′𝑗𝑋𝑗 , where ∑𝑝𝑗=1𝛽′𝑗𝑋𝑗 is the Euclidean distance.

### 9.2 Support Vector Classifier

### 9.3 Feature Expansion and the SVM

### 9.4 Example and Comparison with Logistic Regression

### 9.5. SVMs in R
