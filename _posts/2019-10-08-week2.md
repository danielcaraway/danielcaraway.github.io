---
layout: single
title: 'WK2'
---

## Readings

## 2.2 Tokenization
* To get computers to understand language, we need to turn things into numbers they can count

### What to count
* Tokens -- assemble a set of rules about grouping characters
* First, split into sentences (with punctuation)
* Second, split into words (with spaces)
* Which tokenizer is best? It depends on context & task
* N-Gram: Multiword Tokens
  * Uni-grams
  * Bi-grams
  * Tri-grams

#### Challenges with Tokenization
* When whitespace is absent (for example, in a URL, other languages etc.) tokenization is hard
* Lowercase vs. Uppercase -- which to choose?
* Same word means different things (bank and bank)
* Inflection (dishwasher, dishwashers)

#### Word Sense Disambiguation (WSD)
* use word context to decide the word sense
* so far doesn't help search engines significantly
* not widely used in text mining


### How to count